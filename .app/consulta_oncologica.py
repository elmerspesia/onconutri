import streamlit as st
from transformers import pipeline

@st.cache_resource
def load_chat_model():
    # Modelo mais leve e rÃ¡pido (nÃ£o precisa de API key)
    return pipeline("text2text-generation", model="google/flan-t5-large")

def show_form():
    st.title("ğŸ©º FormulÃ¡rio de Consulta OncolÃ³gica")

    st.markdown("**Responda ao formulÃ¡rio com informaÃ§Ãµes do paciente.**")

    idade = st.selectbox("Idade", ["<30", "30-45", "46-60", ">60"])
    genero = st.selectbox("GÃªnero", ["Masculino", "Feminino", "Outro"])
    tabagismo = st.selectbox("Ã‰ fumante?", ["Nunca fumou", "Fumante ocasional", "Fumante regular", "Ex-fumante"])
    alcool = st.selectbox("Consumo de Ã¡lcool", ["Nunca", "Social", "Frequente", "Abuso"])
    historico_familiar = st.selectbox("HistÃ³rico familiar de cÃ¢ncer?", ["Nenhum", "Sim - parentes de 1Âº grau", "Sim - parentes distantes"])
    dieta = st.selectbox("Tipo de dieta predominante", ["Rica em vegetais e fibras", "Mista com carnes processadas", "Industrializada", "Vegetariana"])
    atividade_fisica = st.selectbox("FrequÃªncia de atividade fÃ­sica", ["SedentÃ¡rio", "1-2x por semana", "3+ vezes/semana"])
    exposicao = st.selectbox("ExposiÃ§Ã£o a toxinas (agrotÃ³xicos, radiaÃ§Ã£o)", ["Nenhuma", "Moderada", "Alta"])
    estresse = st.selectbox("NÃ­vel de estresse diÃ¡rio", ["Baixo", "MÃ©dio", "Alto"])

    if st.button("ğŸ” Processar Consulta"):
        respostas = f"""
        Idade: {idade}, GÃªnero: {genero}, Tabagismo: {tabagismo}, Ãlcool: {alcool},
        HistÃ³rico familiar: {historico_familiar}, Dieta: {dieta},
        Atividade fÃ­sica: {atividade_fisica}, ExposiÃ§Ã£o: {exposicao}, Estresse: {estresse}.
        """

        prompt = f"""
        Com base neste perfil do paciente, avalie o risco de cÃ¢ncer mais provÃ¡vel e recomende estratÃ©gias de prevenÃ§Ã£o alimentar e de estilo de vida.
        Responda em portuguÃªs e de forma objetiva. Perfil: {respostas}
        """

        st.subheader("ğŸ“„ Resultado da Consulta OncolÃ³gica")
        try:
            modelo = load_chat_model()
            resultado = modelo(prompt, max_new_tokens=300)[0]['generated_text']
            st.success("âœ… Consulta processada com sucesso.")
            st.markdown(f"**ğŸ“‹ DiagnÃ³stico e RecomendaÃ§Ã£o:**\n\n{resultado}")
        except Exception as e:
            st.error("âŒ Ocorreu um erro ao processar a consulta. Por favor, tente novamente.")
            st.exception(e)
